{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3982040-2b54-42fd-9e6c-4ad43c869a18",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h1 style=\"color: red;\">Anexo 7</h1>\n",
    "    <h3>Proyecto 7: Creación de Words Embedding Nivel 3</h3>\n",
    "    <hr/>\n",
    "    <p style=\"text-align: right;\">Mg. Luis Felipe Bustamante Narváez</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ebc87a-4b94-45dc-9c29-1964ad2163ec",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227bdfa-f74d-4d31-ad13-960be962eca4",
   "metadata": {},
   "source": [
    "Es un modelo que se utiliza para aprender representaciones vectoriales de palabras. Estas representaciones pueden capturar muchas propiedades lingüisticas de las palabras, como su significado semántico, gramatical y hasta contextual.\n",
    "\n",
    "Para este tercer ejemplo, usaremos 100 textos aleatorios, desde 20.000 caracteres hasta más de 1.500.000 caracteres, los cuales suman para el entrenamiento un total de ---- caracteres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a93808b-0e6d-4cfe-8f0a-a1ca763c8246",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bee20013-3b98-41e7-b41f-8f6d4e0e4cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf2 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e31a9df5-d343-42e5-ac12-0f5503435813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8015a81a-229c-4d09-9fd7-e2c628aa69d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "import PyPDF2\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e4ad60-864b-4a7c-ba7d-2c089e90e021",
   "metadata": {},
   "source": [
    "## Cargamos el documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51ac423b-8f6c-466a-895e-7f2325d61b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_texto_desde_pdf(ruta_archivo):\n",
    "    with open(ruta_archivo, 'rb') as archivo:\n",
    "        lector = PyPDF2.PdfReader(archivo)\n",
    "        texto = ''\n",
    "        for pagina in range(len(lector.pages)):\n",
    "            texto += lector.pages[pagina].extract_text()\n",
    "        return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd57b195-3ed8-4ac7-a126-4a4e77ff54da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_carpeta = 'Entrenamiento_Word2Vec/textos'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4693d8c9-7750-41a1-a270-f8315b3a5501",
   "metadata": {},
   "source": [
    "## Guardamos todos los textos en una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7cef935-7a73-4ce6-adab-f817e3871ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [12:46<00:00,  7.66s/it]\n"
     ]
    }
   ],
   "source": [
    "todos_los_textos = []\n",
    "for archivo in tqdm(os.listdir(ruta_carpeta)):\n",
    "    if archivo.endswith('.pdf'):\n",
    "        ruta_completa = os.path.join(ruta_carpeta, archivo)\n",
    "        try:\n",
    "            documento = extraer_texto_desde_pdf(ruta_completa)\n",
    "            todos_los_textos.append(documento)\n",
    "        except Exception as e:\n",
    "            print(f'Error al procesar {archivo}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59d5dec5-02e5-45ef-8e62-76f5f5d14593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(todos_los_textos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4873080-2e17-4abb-9dbd-2ed569809ba0",
   "metadata": {},
   "source": [
    "## Procesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60e5886-b82d-42f0-a1d4-9fb982875dc5",
   "metadata": {},
   "source": [
    "El objetivo del procesamiento es convertir el documento en una lista de frases, y cada frase en una lista de palabras, eliminando signos de puntuación y convirtiendo todo a minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32cb8d0f-e795-4cb5-bb1f-5ab47fceca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el documento en frases usando la coma como separador\n",
    "frases_totales = []\n",
    "caracteres = 0\n",
    "\n",
    "for documento in todos_los_textos:\n",
    "    caracteres = caracteres + len(documento)\n",
    "    frases = documento.split(',')\n",
    "    frases_totales.extend(frases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a3f1353-e953-4b18-858f-0bb30eabc4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de caracteres: 240745048\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el número de caracteres totales\n",
    "print(f'Número de caracteres: {caracteres}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "493ac75f-8a34-4bc7-a6b0-0ed95abab464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número de frases totales es de: 1980300\n"
     ]
    }
   ],
   "source": [
    "# Mostramos el número de oraciones totales\n",
    "print(f'El número de frases totales es de: {len(frases_totales)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea9c0adc-728e-4c2b-878d-39de63dabc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninfluyendo en el desarrollo de la humanidad en múltiples aspectos. A medida que avanzamos en el\\nconocimiento'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos un ejemplo\n",
    "frases_totales[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d330c2c-7dea-4d11-83a4-4b95f4204c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' nuevas perspectivas emergen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos un ejemplo\n",
    "frases_totales[3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7c06bf6-d070-49db-99c9-2f18003ab90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiamos las frases\n",
    "frases_limpias = []\n",
    "for frase in frases_totales:\n",
    "    #Eliminamos la puntuación y dividimos por espacios\n",
    "    tokens = frase.translate(str.maketrans('','',string.punctuation)).split()\n",
    "    #print(tokens)  #para mostrar qué ha hecho hasta aquí\n",
    "    #Convertimos a minúsculas\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    #print(tokens)  #para mostrar qué ha hecho hasta aquí\n",
    "    if tokens:\n",
    "        frases_limpias.append(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94f86ca3-a3cb-4c0d-9a78-0f4e2d28a3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['influyendo',\n",
       " 'en',\n",
       " 'el',\n",
       " 'desarrollo',\n",
       " 'de',\n",
       " 'la',\n",
       " 'humanidad',\n",
       " 'en',\n",
       " 'múltiples',\n",
       " 'aspectos',\n",
       " 'a',\n",
       " 'medida',\n",
       " 'que',\n",
       " 'avanzamos',\n",
       " 'en',\n",
       " 'el',\n",
       " 'conocimiento']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos los resultados\n",
    "frases_limpias[500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e4d22-93ad-40f5-9108-c137fbb45f7c",
   "metadata": {},
   "source": [
    "## CPU disponibles en mi PC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd5660-9342-4ade-a01c-70723c218a22",
   "metadata": {},
   "source": [
    "En este apartado, observaremos la cantidad de núcleos de procesamiento tiene nuestro computador para el trabajo en NPL. Como este modelo requiere de más gasto computacional, es bueno identificar este dato, para ser eficientes en el entrenamiento, y evitar relentizar el equipo u otros procesos en paralelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad83f111-ac14-4205-a1d8-dafa84134870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mi equipo tiene 8 CPU´s\n"
     ]
    }
   ],
   "source": [
    "def numero_de_cpus():\n",
    "    return os.cpu_count()\n",
    "\n",
    "print(f'Mi equipo tiene {numero_de_cpus()} CPU´s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ff463-a2bb-43e5-a3f0-dee84ce5caad",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ba4a159-f20d-4ef1-b95a-ef2d220f50af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=frases_limpias,\n",
    "                vector_size=500,\n",
    "                window=5,\n",
    "                min_count=1,\n",
    "                workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acff46e-5c0b-44a7-a55f-713993a934de",
   "metadata": {},
   "source": [
    "### Explicación:\n",
    "\n",
    "- sentences: Es la lista de palabras que vamos a vectorizar\n",
    "- vector_size: Es el tamaño de dimensiones que le daremos al vector\n",
    "- window: Son la cantidad de palabras por encima y por debajo que le darán contexto\n",
    "- min_count: La aparición mínima de una palabra para tenerla en cuenta en el entrenamiento\n",
    "- workers: Cantidad de núcleo de procesador que vamos a invertir en el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd5bae1-0e34-4f02-ae04-724e6c268e02",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae88f78c-5d7f-4b2a-97d5-74a48bea910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos el vector para alguna palabra\n",
    "vector = model.wv['ciencia']\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6b074fc-b590-41da-8a86-1ee061d98dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('principios', 0.3798693120479584),\n",
       " ('tecnología', 0.36144816875457764),\n",
       " ('medicina', 0.2695100009441376),\n",
       " ('biografía', 0.2573043406009674),\n",
       " ('múltiples', 0.2501693665981293),\n",
       " ('múltiplesdimensiones', 0.2432234287261963),\n",
       " ('economía', 0.24100026488304138),\n",
       " ('relevantebiografía', 0.22121661901474),\n",
       " ('vidas', 0.20030616223812103),\n",
       " ('han', 0.19419683516025543)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos las palabras cercanas\n",
    "palabras_cercanas = model.wv.most_similar('ciencia', topn=10)\n",
    "palabras_cercanas\n",
    "# Es probable que la similitud falle por tener tan pocas palabras en el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d19b8ba7-7a3e-4c43-8199-fa78bd75fff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('enseñanza', 0.8446722030639648),\n",
       " ('educación', 0.4766235649585724),\n",
       " ('crisis', 0.4423055946826935),\n",
       " ('financieras', 0.4304288625717163),\n",
       " ('cultural', 0.37052440643310547),\n",
       " ('videojuegos', 0.35479822754859924),\n",
       " ('actualidad', 0.29080677032470703),\n",
       " ('gastronomía', 0.25270408391952515),\n",
       " ('fundamentales', 0.2261238843202591),\n",
       " ('aportes', 0.2239352911710739)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos las palabras cercanas\n",
    "palabras_cercanas = model.wv.most_similar('sistemas', topn=10)\n",
    "palabras_cercanas\n",
    "# Es probable que la similitud falle por tener tan pocas palabras en el texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660889ed-38eb-42bb-9d0a-b5e459ee345f",
   "metadata": {},
   "source": [
    "## Guardar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2dfc0fc7-e106-4b65-87c7-10c8977b6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Entrenamiento_Word2Vec/100textos.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8c6e49-42d0-441f-b02b-2f30d32adb76",
   "metadata": {},
   "source": [
    "## Cargar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b044d59-a128-4709-8673-638a318bd505",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_cargado = Word2Vec.load('Entrenamiento_Word2Vec/100textos.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6cd3986e-bb0a-4a9b-af82-3ea5b31ef893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emocional', 0.6839113831520081),\n",
       " ('industria', 0.29906395077705383),\n",
       " ('cultura', 0.2832416296005249),\n",
       " ('másimportantes', 0.24401310086250305),\n",
       " ('expertos', 0.2300969511270523),\n",
       " ('energía', 0.22232283651828766),\n",
       " ('géneros', 0.22123292088508606),\n",
       " ('importantes', 0.20623861253261566),\n",
       " ('nuclear', 0.19350115954875946),\n",
       " ('descubrimientos', 0.18687954545021057)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probamos con el modelo caragado\n",
    "palabras_cercanas2 = modelo_cargado.wv.most_similar('importancia', topn=10)\n",
    "palabras_cercanas2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784098f3-2751-45de-b67c-7302d613321a",
   "metadata": {},
   "source": [
    "## Guardar Embedido\n",
    "\n",
    "Existen dos maneras, usando .txt sin binarios, y usando .bin con binarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f78ea55-cf0d-4395-9650-32a0a2ad4367",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('Entrenamiento_Word2Vec/100textos_emb.txt', binary=False)\n",
    "model.wv.save_word2vec_format('Entrenamiento_Word2Vec/100textos_emb.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59664b52-f853-4557-b4e0-7298c05f29f5",
   "metadata": {},
   "source": [
    "## Cargar Embedidos\n",
    "\n",
    "Si se carga el .txt, se usa sin binarios; si se carga el .bin, se usa con binarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da56a7c8-519a-4505-a295-5420311b8720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "embedding_cargado_txt = KeyedVectors.load_word2vec_format(\n",
    "    'Entrenamiento_Word2Vec/100textos_emb.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac5741aa-c0da-4136-b34b-aeee639b7b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_cargado_bin = KeyedVectors.load_word2vec_format(\n",
    "    'Entrenamiento_Word2Vec/100textos_emb.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4018364-c596-4092-82a1-da7a9fe6dd0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x1d45dbc0a70>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probamos\n",
    "embedding_cargado_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fbe8212d-477d-4f87-8268-6dbebea6d77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x1d444aa0710>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probamos\n",
    "embedding_cargado_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11664fde-dba1-4625-8679-d08dad7142ad",
   "metadata": {},
   "source": [
    "## Analogías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a646f171-87b7-4874-9508-73cbaa5410ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogics(v1, v2, v3):\n",
    "    simil = embedding_cargado_bin.most_similar(positive=[v1,v3], \n",
    "                                               negative=[v2]\n",
    "                                              )\n",
    "    print(f'{v1} es a {v2}, como {simil[0][0]} es a {v3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2fee7c13-25ae-49b9-956b-f443568103cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "científico es a ciencia, como pensamiento es a cultura\n"
     ]
    }
   ],
   "source": [
    "analogics('científico', 'ciencia', 'cultura')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ec5aedb3-2f15-4200-a359-5531a1580834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "científico es a ciencia, como energía es a nuclear\n"
     ]
    }
   ],
   "source": [
    "analogics('científico', 'ciencia', 'nuclear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8800e478-17b4-420d-b883-8381c99f8322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "científico es a ciencia, como romano es a imperio\n"
     ]
    }
   ],
   "source": [
    "analogics('científico', 'ciencia', 'imperio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c4553b-f65a-4e08-be28-45069cf58070",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982275d7-04d8-4f89-8129-df9eb14e6b9c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <p>Utilizando 100 textos con temáticas aleatorias, se puede observar que las predicciones en las analogías son mucho más reales; chatGPT3 utilizó en su primer entrenamiento 570G en textos, libros y artículos, nuestro entrenamiento utilizó tan solo 82.8M y aún así, encontramos mucha coherencia a la hora de probar la similitud. ¿Qué pasaría si usaramos por lo menos 1.000.000 de textos? </p>\n",
    "    <hr/>\n",
    "    <p style=\"text-align: right;\">Mg. Luis Felipe Bustamante Narváez</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
