{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee468d8-dda9-4ff0-9556-17aa1a4b49ff",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h1 style=\"color: red;\">Anexo A</h1>\n",
    "    <h3>Módulo 1 - Ejemplos</h3>\n",
    "    <hr/>\n",
    "    <p style=\"text-align: right;\">Mg. Luis Felipe Bustamante Narváez</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f368ba5b-4192-4da9-b6cc-99f43f41ca0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ejemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1317706d-f4d8-4f46-a38a-db8c2d26b68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola,', '¿Cómo', 'estás?']\n"
     ]
    }
   ],
   "source": [
    "texto = \"Hola, ¿Cómo estás?\"\n",
    "tokens = texto.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b1a4b5-a18d-48e2-9d35-0a095339c4d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ejemplo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe902d37-4172-48e6-ae14-6d52018bb95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola, ¿cómo estás?\n",
      "['hola,', '¿cómo', 'estás?']\n"
     ]
    }
   ],
   "source": [
    "texto = \"Hola, ¿Cómo estás?\"\n",
    "texto = texto.lower()\n",
    "print(texto)\n",
    "tokens = texto.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3845bf3a-4328-4dfe-baf1-fa6dd7c0b232",
   "metadata": {},
   "source": [
    "## Ejemplo 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4a505c8-f212-4358-8e9f-c18d977f8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"El gato es negro y el perro es blanco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a21088b8-22c8-410f-85f6-92d8338af8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a71e6894-31bf-41cd-89e1-d9d17c13b4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\luis_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54a255a9-0ff1-48e0-8f2c-80334ab90c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6b0ad15-477a-4a24-bf57-13004f723a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tendrán', 'tuvierais', 'sois', 'tenías', 'sintiendo', 'del', 'tuyo', 'tendrá', 'muchos', 'nuestro', 'seremos', 'algunas', 'seríais', 'les', 'ellos', 'habría', 'sean', 'hubiera', 'quienes', 'mías', 'vuestro', 'estén', 'soy', 'hubiesen', 'vosotros', 'siente', 'vosotras', 'otros', 'algo', 'están', 'sí', 'nuestras', 'tuyas', 'hayáis', 'no', 'ti', 'fuera', 'habré', 'habidas', 'sentida', 'estuvieron', 'tus', 'tuvieran', 'estuvimos', 'con', 'tenéis', 'tenidas', 'entre', 'esto', 'tenemos', 'habidos', 'me', 'seas', 'tendríais', 'sobre', 'hayas', 'nos', 'este', 'fuerais', 'nosotras', 'habido', 'es', 'estabas', 'tuviese', 'porque', 'eran', 'mi', 'estuviera', 'ni', 'mí', 'las', 'tuviera', 'estada', 'tuvisteis', 'unos', 'se', 'esa', 'nada', 'seréis', 'su', 'estuvo', 'tenidos', 'o', 'tenía', 'tengas', 'sentidos', 'habremos', 'estuviesen', 'estuvieran', 'habida', 'tuvieseis', 'esté', 'habías', 'hubisteis', 'ya', 'habrás', 'estaban', 'otra', 'un', 'tuve', 'tuvimos', 'eres', 'otro', 'contra', 'una', 'tuviste', 'hubiese', 'estaréis', 'había', 'estoy', 'ese', 'tendremos', 'fui', 'estemos', 'sin', 'fuéramos', 'tengamos', 'estas', 'estuvieras', 'estaría', 'fuisteis', 'cuando', 'estuvisteis', 'estuvieses', 'tiene', 'hubiéramos', 'teníamos', 'estuviese', 'algunos', 'otras', 'estuviésemos', 'tendré', 'estuviste', 'estarías', 'hay', 'habríais', 'éramos', 'tuviesen', 'donde', 'suyos', 'hayamos', 'habrían', 'tuya', 'hubierais', 'estos', 'para', 'nuestros', 'suyo', 'cual', 'la', 'esas', 'tenga', 'hubo', 'tienen', 'habíais', 'tengo', 'los', 'habiendo', 'estar', 'estás', 'tened', 'nuestra', 'estabais', 'estará', 'tuvieses', 'tendréis', 'a', 'serían', 'tendríamos', 'habrán', 'mía', 'hubieses', 'ha', 'estuviéramos', 'tendrías', 'estuve', 'qué', 'hubiste', 'han', 'todos', 'teníais', 'suya', 'fuese', 'hubiésemos', 'tienes', 'todo', 'esos', 'eso', 'habréis', 'fueran', 'más', 'quien', 'tuvieras', 'él', 'era', 'vuestros', 'estados', 'desde', 'tendrían', 'mío', 'fueseis', 'hemos', 'sus', 'estarían', 'vuestras', 'estado', 'seamos', 'eras', 'teniendo', 'tendría', 'estaríais', 'seré', 'sentido', 'seáis', 'habían', 'tenida', 'hayan', 'fue', 'suyas', 'durante', 'tuyos', 'hubieran', 'fueron', 'estamos', 'son', 'vuestra', 'fueras', 'estábamos', 'tuviéramos', 'mucho', 'hube', 'en', 'estaré', 'fuiste', 'has', 'que', 'tanto', 'mis', 'sería', 'serás', 'sentidas', 'habéis', 'fueses', 'tuvieron', 'ella', 'el', 'te', 'habrías', 'poco', 'fuesen', 'hasta', 'tengan', 'hubieras', 'estad', 'somos', 'seríamos', 'será', 'fuimos', 'ellas', 'estuvieseis', 'estando', 'tenido', 'tengáis', 'uno', 'estaríamos', 'muy', 'serán', 'erais', 'por', 'hubieseis', 'estarás', 'habríamos', 'tuvo', 'haya', 'tuviésemos', 'está', 'ante', 'nosotros', 'tu', 'pero', 'antes', 'míos', 'estadas', 'serías', 'estéis', 'he', 'estaba', 'tú', 'tenían', 'al', 'habíamos', 'sea', 'sentid', 'esta', 'estuvierais', 'lo', 'de', 'como', 'e', 'le', 'estés', 'tendrás', 'estáis', 'yo', 'también', 'os', 'estaremos', 'fuésemos', 'estarán', 'hubimos', 'hubieron', 'habrá', 'y'}\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('spanish'))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94b3b3d9-49b2-4f69-8c3e-f4e8a2648cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El', 'gato', 'es', 'negro', 'y', 'el', 'perro', 'es', 'blanco']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(texto)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b26fc14c-55ca-4e1f-b215-c9851022aa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El', 'gato', 'negro', 'perro', 'blanco']\n"
     ]
    }
   ],
   "source": [
    "textoFilter = [word for word in tokens if not word in stopWords]\n",
    "print(textoFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12d5c833-dda1-434f-acb2-ef3900d1e225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gato', 'negro', 'perro', 'blanco']\n"
     ]
    }
   ],
   "source": [
    "texto = texto.lower()\n",
    "tokens = word_tokenize(texto)\n",
    "textoFilter = [word for word in tokens if not word in stopWords]\n",
    "print(textoFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b9277dd-94f6-444c-8522-e9b42cf6a357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tendrán', 'tuvierais', 'sois', 'tenías', 'sintiendo', 'del', 'tuyo', 'tendrá', 'muchos', 'nuestro', 'seremos', 'algunas', 'seríais', 'les', 'ellos', 'habría', 'sean', 'hubiera', 'quienes', 'mías', 'vuestro', 'estén', 'soy', 'hubiesen', 'vosotros', 'siente', 'vosotras', 'otros', 'algo', 'están', 'sí', 'nuestras', 'tuyas', 'hayáis', 'no', 'ti', 'fuera', 'habré', 'habidas', 'sentida', 'estuvieron', 'tus', 'tuvieran', 'estuvimos', 'con', 'tenéis', 'tenidas', 'entre', 'esto', 'tenemos', 'habidos', 'me', 'seas', 'tendríais', 'sobre', 'hayas', 'nos', 'este', 'fuerais', 'nosotras', 'habido', 'es', 'estabas', 'tuviese', 'porque', 'eran', 'mi', 'estuviera', 'ni', 'mí', 'las', 'tuviera', 'estada', 'tuvisteis', 'unos', 'se', 'esa', 'nada', 'seréis', 'su', 'estuvo', 'tenidos', 'o', 'tenía', 'tengas', 'sentidos', 'habremos', 'estuviesen', 'estuvieran', 'habida', 'tuvieseis', 'esté', 'habías', 'hubisteis', 'ya', 'habrás', 'estaban', 'otra', 'un', 'tuve', 'tuvimos', 'hola', 'eres', 'otro', 'contra', 'una', 'tuviste', 'hubiese', 'estaréis', 'había', 'estoy', 'ese', 'tendremos', 'fui', 'estemos', 'sin', 'fuéramos', 'tengamos', 'estas', 'estuvieras', 'estaría', 'fuisteis', 'cuando', 'estuvisteis', 'estuvieses', 'tiene', 'hubiéramos', 'teníamos', 'estuviese', 'algunos', 'otras', 'estuviésemos', 'tendré', 'estuviste', 'estarías', 'hay', 'habríais', 'éramos', 'tuviesen', 'donde', 'suyos', 'hayamos', 'habrían', 'tuya', 'hubierais', 'estos', 'para', 'nuestros', 'suyo', 'cual', 'la', 'esas', 'tenga', 'hubo', 'tienen', 'habíais', 'tengo', 'los', 'habiendo', 'estar', 'estás', 'tened', 'nuestra', 'estabais', 'estará', 'tuvieses', 'tendréis', 'a', 'serían', 'tendríamos', 'habrán', 'mía', 'hubieses', 'ha', 'estuviéramos', 'tendrías', 'estuve', 'qué', 'hubiste', 'han', 'todos', 'teníais', 'suya', 'fuese', 'hubiésemos', 'tienes', 'todo', 'ejemplo', 'esos', 'eso', 'habréis', 'fueran', 'más', 'quien', 'tuvieras', 'él', 'era', 'vuestros', 'estados', 'desde', 'tendrían', 'mío', 'fueseis', 'hemos', 'sus', 'estarían', 'vuestras', 'estado', 'seamos', 'eras', 'teniendo', 'tendría', 'estaríais', 'seré', 'sentido', 'seáis', 'habían', 'tenida', 'hayan', 'fue', 'suyas', 'durante', 'tuyos', 'hubieran', 'fueron', 'estamos', 'son', 'vuestra', 'fueras', 'estábamos', 'tuviéramos', 'mucho', 'hube', 'en', 'estaré', 'fuiste', 'has', 'que', 'tanto', 'mis', 'sería', 'serás', 'sentidas', 'habéis', 'fueses', 'tuvieron', 'ella', 'el', 'te', 'habrías', 'poco', 'fuesen', 'hasta', 'tengan', 'hubieras', 'estad', 'somos', 'seríamos', 'será', 'fuimos', 'ellas', 'estuvieseis', 'estando', 'tenido', 'tengáis', 'uno', 'estaríamos', 'muy', 'serán', 'erais', 'por', 'hubieseis', 'estarás', 'habríamos', 'tuvo', 'haya', 'tuviésemos', 'está', 'prueba', 'ante', 'nosotros', 'tu', 'pero', 'antes', 'míos', 'estadas', 'serías', 'estéis', 'he', 'estaba', 'tú', 'tenían', 'al', 'habíamos', 'sea', 'sentid', 'esta', 'estuvierais', 'lo', 'de', 'como', 'e', 'le', 'estés', 'tendrás', 'estáis', 'yo', 'también', 'os', 'estaremos', 'fuésemos', 'estarán', 'hubimos', 'hubieron', 'habrá', 'y'}\n"
     ]
    }
   ],
   "source": [
    "newStopWords = {\"ejemplo\", \"prueba\", \"hola\"}\n",
    "stopWords.update(newStopWords)\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f707c345-5376-4081-9180-eb617df877f1",
   "metadata": {},
   "source": [
    "## Ejemplo 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4ed2048-a5e1-40b8-9cc9-652fefca6522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\luis_\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2af4a422-dd1c-4373-b036-e898bc1ec2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ed54d63-58b7-4bee-a147-17dab07f6539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camin\n",
      "camin\n",
      "camin\n"
     ]
    }
   ],
   "source": [
    "# Creamos el Stemmer en español\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "# Probamos en la palabra \"caminar\"\n",
    "print(stemmer.stem('caminando'))\n",
    "print(stemmer.stem('caminar'))\n",
    "print(stemmer.stem('caminó'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b210574d-5e90-4041-b730-4901cb556a59",
   "metadata": {},
   "source": [
    "## Ejemplo 5\n",
    "La lemmatization no está disponible en español para nltk, entonces usamos spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76d6b565-3eff-4a01-954c-929fa7944340",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (2.2.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\luis_\\anaconda3\\envs\\notebook\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a01ea57e-263d-417e-b5f7-50d202c316de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "     ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.9 MB 5.6 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.6/12.9 MB 7.9 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.5/12.9 MB 8.1 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.3/12.9 MB 8.2 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.9/12.9 MB 8.2 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.7/12.9 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.3/12.9 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.9 MB 8.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.9/12.9 MB 7.8 MB/s eta 0:00:00\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6380707d-7e3c-4789-9a42-59b33c99fb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caminando -> caminar\n",
      "caminó -> caminar\n",
      "caminar -> caminar\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargamos el modelo en español\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Creamos un documento\n",
    "doc = nlp(\"caminando caminó caminar\")\n",
    "\n",
    "# Imprimimos el texto y el lema de cada token\n",
    "for token in doc:\n",
    "    print(token.text, \"->\", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaf94db-c7c6-493b-8d90-974673372b67",
   "metadata": {},
   "source": [
    "## Ejemplo 6\n",
    "Implementamos algo de nklt para lemmatizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c97b8ba-617a-4052-b0e5-2fde919ef620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: Los niños están corriendo en el parque mientras los perros jugaban.\n",
      "Texto lematizado: el niño estar correr en el parque mientras el perro jugar .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Descargar modelo en español de spaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Texto de ejemplo\n",
    "texto = \"Los niños están corriendo en el parque mientras los perros jugaban.\"\n",
    "\n",
    "# Tokenización con nltk\n",
    "tokens = word_tokenize(texto)\n",
    "\n",
    "# Lematización con spaCy\n",
    "lemmas = [token.lemma_ for token in nlp(\" \".join(tokens))]\n",
    "\n",
    "print(\"Texto original:\", texto)\n",
    "print(\"Texto lematizado:\", \" \".join(lemmas))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bdefe5-ffeb-49a8-a26a-bf524f0d32e5",
   "metadata": {},
   "source": [
    "## Ejemplo 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b55a24-611d-4c9f-9c9e-7bcd266dd483",
   "metadata": {},
   "source": [
    "### CPU disponibles en mi PC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68407073-918f-4518-a0b7-fb614ff75fd2",
   "metadata": {},
   "source": [
    "En este apartado, observaremos la cantidad de núcleos de procesamiento tiene nuestro computador para el trabajo en NPL. Ya que en los siguientes ejemplos prácticos, usaremos el rendimiento del equipo para entrenar los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd16895-463e-4da0-a67b-7e9a529ba12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mi equipo tiene 8 CPU´s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def numero_de_cpus():\n",
    "    return os.cpu_count()\n",
    "\n",
    "print(f'Mi equipo tiene {numero_de_cpus()} CPU´s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
